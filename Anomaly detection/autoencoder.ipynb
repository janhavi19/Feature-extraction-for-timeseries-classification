{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5vzj7UuSFM5l"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.layers import Input, Dense,RepeatVector, TimeDistributed, Dense, Dropout, LSTM\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, recall_score, accuracy_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix, recall_score, accuracy_score, precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-KRIOhKsHCWI"
   },
   "outputs": [],
   "source": [
    "def conv_array(array):\n",
    "    result=[]\n",
    "    for r in array:\n",
    "        result.append(r)\n",
    "    return np.asarray(result)\n",
    "\n",
    "def SSIMLoss(y_true, y_pred):\n",
    "    return 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred,1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CPQpVuDBFZxA"
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"./DataSignal.pkl\")\n",
    "df_f40=data.loc[((data['Status'] == \"OK\")|(data['Status'] == \"NOK\"))&(data['Model'] == \"F40\")]\n",
    "\n",
    "df_OK = df_f40.loc[(df_f40['Status']==\"OK\")]\n",
    "df_NOK = df_f40.loc[(df_f40['Status']==\"NOK\")]\n",
    "\n",
    "x_train = conv_array(df_OK['Acc'])\n",
    "x_test = conv_array(df_NOK['Acc'])\n",
    "y_train = np.asarray([1]*(len(df_OK['Status'])))\n",
    "y_test = np.asarray([0]*(len(df_NOK['Status'])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VvrGMY4oJiCT",
    "outputId": "cee7be70-2c49-4e60-c6a2-759e76634331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2313, 3500)\n",
      "(2313,)\n",
      "(141, 3500)\n",
      "(141,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OYGO1uUUveTJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pos = conv_array(df_f40['Pos'])\n",
    "\n",
    "\n",
    "y = conv_array(df_f40['Status'])\n",
    "lb = LabelEncoder()\n",
    "y = lb.fit_transform(y)\n",
    "\n",
    "\n",
    "X_t, X_te, y_t, y_te = train_test_split(pos, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HOIMwX9eiKQk"
   },
   "outputs": [],
   "source": [
    "nb_epoch = 50\n",
    "batch_size = 10\n",
    "input_dim =3500 #num of columns, 30\n",
    "encoding_dim = 1000\n",
    "hidden_dim_1 = int(encoding_dim / 2) #\n",
    "hidden_dim_2=90 \n",
    "learning_rate = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MFTKjApAGt7E",
    "outputId": "d4a4c9af-f014-4449-b7df-11b451e652b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 3500)]            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1000)              3501000   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 90)                45090     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 500)               45500     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1000)              501000    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3500)              3503500   \n",
      "=================================================================\n",
      "Total params: 8,096,590\n",
      "Trainable params: 8,096,590\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#input Layer\n",
    "input_layer = tf.keras.layers.Input(shape=(input_dim, ))\n",
    "#Encoder\n",
    "encoder = tf.keras.layers.Dense(encoding_dim, activation=\"tanh\",activity_regularizer=tf.keras.regularizers.l2(learning_rate))(input_layer)\n",
    "encoder=tf.keras.layers.Dropout(0.2)(encoder)\n",
    "encoder = tf.keras.layers.Dense(hidden_dim_1, activation='relu')(encoder)\n",
    "encoder = tf.keras.layers.Dense(hidden_dim_2, activation=tf.nn.leaky_relu)(encoder)\n",
    "# Decoder\n",
    "decoder = tf.keras.layers.Dense(hidden_dim_1, activation='relu')(encoder)\n",
    "decoder=tf.keras.layers.Dropout(0.2)(decoder)\n",
    "decoder = tf.keras.layers.Dense(encoding_dim, activation='relu')(decoder)\n",
    "decoder = tf.keras.layers.Dense(input_dim, activation='tanh')(decoder)\n",
    "#Autoencoder\n",
    "autoencoder = tf.keras.Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PlGaFfGCOoKw"
   },
   "outputs": [],
   "source": [
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mse',\n",
    "                    optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zmgKyctlRoMo",
    "outputId": "1e253c52-554e-49a8-cc09-0494f58353de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "165/165 [==============================] - 12s 66ms/step - loss: nan - accuracy: 0.9757 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 2/50\n",
      "165/165 [==============================] - 10s 61ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 3/50\n",
      "165/165 [==============================] - 11s 64ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 4/50\n",
      "165/165 [==============================] - 12s 75ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 5/50\n",
      "165/165 [==============================] - 11s 64ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 6/50\n",
      "165/165 [==============================] - 11s 67ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 7/50\n",
      "165/165 [==============================] - 10s 63ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 8/50\n",
      "165/165 [==============================] - 12s 74ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 9/50\n",
      "165/165 [==============================] - 11s 70ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 10/50\n",
      "165/165 [==============================] - 11s 65ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 11/50\n",
      "165/165 [==============================] - 11s 66ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 12/50\n",
      "165/165 [==============================] - 11s 68ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 13/50\n",
      "165/165 [==============================] - 11s 65ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 14/50\n",
      "165/165 [==============================] - 11s 64ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 15/50\n",
      "165/165 [==============================] - 14s 84ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 16/50\n",
      "165/165 [==============================] - 11s 70ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 17/50\n",
      "165/165 [==============================] - 12s 71ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 18/50\n",
      "165/165 [==============================] - 11s 69ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 19/50\n",
      "165/165 [==============================] - 11s 64ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 20/50\n",
      "165/165 [==============================] - 10s 62ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 21/50\n",
      "165/165 [==============================] - 11s 68ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 22/50\n",
      "165/165 [==============================] - 11s 64ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 23/50\n",
      "165/165 [==============================] - 11s 67ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 24/50\n",
      "165/165 [==============================] - 12s 71ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 25/50\n",
      "165/165 [==============================] - 10s 62ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 26/50\n",
      "165/165 [==============================] - 10s 63ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 27/50\n",
      "165/165 [==============================] - 10s 63ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 28/50\n",
      "165/165 [==============================] - 11s 65ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 29/50\n",
      "165/165 [==============================] - 11s 65ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 30/50\n",
      "165/165 [==============================] - 11s 67ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 31/50\n",
      "165/165 [==============================] - 10s 62ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 32/50\n",
      "165/165 [==============================] - 11s 67ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 33/50\n",
      "165/165 [==============================] - 11s 69ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 34/50\n",
      "165/165 [==============================] - 10s 62ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 35/50\n",
      "165/165 [==============================] - 12s 72ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 36/50\n",
      "165/165 [==============================] - 13s 78ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 37/50\n",
      "165/165 [==============================] - 12s 70ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 38/50\n",
      "165/165 [==============================] - 11s 65ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 39/50\n",
      "165/165 [==============================] - 12s 72ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 40/50\n",
      "165/165 [==============================] - 11s 65ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 41/50\n",
      "165/165 [==============================] - 11s 66ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 42/50\n",
      "165/165 [==============================] - 11s 65ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.9988\n",
      "Epoch 43/50\n",
      "163/165 [============================>.] - ETA: 0s - loss: nan - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(X_t , X_t ,\n",
    "                    epochs=50,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(X_te, X_te),\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(autoencoder.history.history['loss'], linewidth=2, label='Train')\n",
    "plt.plot(autoencoder.history.history['val_loss'], linewidth=2, label='Valid')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss funtion shows reptative behavior of input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oigVt2y1nd3H"
   },
   "outputs": [],
   "source": [
    "test_x_predictions = autoencoder.predict(X_te)\n",
    "mse = np.mean(np.power(X_te - test_x_predictions, 2), axis=1)\n",
    "error_df = pd.DataFrame({'Reconstruction_error': mse,\n",
    "                        'True_class': y_te})\n",
    "error_df.loc[(error_df['True_class']==0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXH4PxARu-Lz"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "gSvGk8USq2EN",
    "outputId": "eccfd803-d2b3-48f6-eec7-517842938964"
   },
   "outputs": [],
   "source": [
    "threshold_fixed =0.0004\n",
    "\n",
    "LABELS = [\"NOK\",\"OK\"]\n",
    "pred_y = [1 if e > threshold_fixed else 0 for e in error_df.Reconstruction_error.values]\n",
    "error_df['pred'] =pred_y\n",
    "conf_matrix = confusion_matrix(error_df.True_class, pred_y)\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.show()\n",
    "# print Accuracy, precision and recall\n",
    "print(\" Accuracy: \",accuracy_score(error_df['True_class'], error_df['pred']))\n",
    "print(\" Recall: \",recall_score(error_df['True_class'], error_df['pred']))\n",
    "print(\" Precision: \",precision_score(error_df['True_class'], error_df['pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = error_df.groupby('True_class')\n",
    "fig, ax = plt.subplots()\n",
    "for name, group in groups:\n",
    "    ax.plot(group.index, group.Reconstruction_error, marker='o', ms=3.5, linestyle='',\n",
    "            label= \"Normal\" if name == 1 else \"Anomaly\")\n",
    "ax.hlines(threshold_fixed, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\n",
    "ax.legend()\n",
    "plt.title(\"Reconstruction error for normal and abnormal data\")\n",
    "plt.ylabel(\"Reconstruction error\")\n",
    "plt.xlabel(\"Data point index\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "autoencoder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
